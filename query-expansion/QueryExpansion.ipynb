{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Just some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import common as cm\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Simple search engine (for 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Get acquainted with the below class. There are several TODOs. However, DO NOT complete them now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaai', 'about', 'academic', 'access', 'acquired', 'acquisition', 'action', 'activity', 'actual', 'adaptive', 'add', 'advance', 'agricultural', 'aha', 'aim', 'alert', 'algorithm', 'all', 'analysis', 'and', 'announcement', 'answer', 'anyone', 'application', 'applied', 'apply', 'applying', 'approach', 'approache', 'april', 'archive', 'are', 'area', 'areas', 'article', 'artificial', 'asked', 'august', 'author', 'automated', 'automatically', 'autonomous', 'available', 'awards', 'backend', 'backgammon', 'baldi', 'based', 'basic', 'bayesian']\n"
     ]
    }
   ],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        ### keeps unique terms (SORTED!)\n",
    "        self.terms = self.loadTerms(\"terms.txt\")\n",
    "        self.idfs = [] ### IDF coefficients\n",
    "        \n",
    "        self.corM = [] ### a correlation matrix\n",
    "        self.corM_A = [] ### helpful matrices\n",
    "        self.corM_A_norm = []\n",
    "        self.corM_transposed = []\n",
    "        self.corM_final = []\n",
    "        \n",
    "        self.idfs_dict = {}\n",
    "        \n",
    "    ### load terms\n",
    "    def loadTerms(self, fileName):\n",
    "        file = open(fileName,'r', encoding='utf-8-sig')\n",
    "        k = [self.proc(s) for s in file.readlines()]\n",
    "        k.sort()\n",
    "        return k\n",
    "\n",
    "    ### ignore it\n",
    "    def proc(self, s):\n",
    "        if s[-1] == '\\n': return s[:-1]\n",
    "        else: return s\n",
    "    \n",
    "    ### How common is word across a set of documents\n",
    "    ### IDF = log(N / df(w)); N - total number of documents, df(w) - frequency of documents containing word 'w'\n",
    "    def computeIDFs(self, documents):\n",
    "        \n",
    "        self.idfs = []\n",
    "        N = len(documents)\n",
    "        df = []\n",
    "        for i, term in enumerate(self.terms):\n",
    "            df.append(0)\n",
    "            for document in documents:\n",
    "                found = False\n",
    "                for token in document.tokens:\n",
    "                    if token == term and not found:\n",
    "                        df[i] += 1\n",
    "                        found = True\n",
    "                    \n",
    "        for freq in df:\n",
    "            self.idfs.append(math.log10(N / freq))\n",
    "            \n",
    "        for term, idf in zip(self.terms, self.idfs):\n",
    "            self.idfs_dict[term] = idf \n",
    "\n",
    "\n",
    "    def computeCorM(self, documents):\n",
    "        for i in range(len(self.terms)):\n",
    "            any = 0\n",
    "            tab = []\n",
    "            for j in range(len(documents)):\n",
    "                tab.append(0)\n",
    "                tab[j] = len([term for term in documents[j].tokens if term == self.terms[i]])\n",
    "            self.corM.append(tab)\n",
    "            \n",
    "        for i in range(len(self.corM)):\n",
    "            sum = 0\n",
    "            for j in range(len(self.corM[i])):\n",
    "                sum += math.pow(self.corM[i][j], 2)\n",
    "            sum = math.sqrt(sum)\n",
    "            for j in range(len(self.corM[i])):\n",
    "                self.corM[i][j] /= sum \n",
    "        \n",
    "        transM = np.transpose(self.corM)\n",
    "        self.corM = np.matmul(self.corM, transM)\n",
    "        for i in range(len(self.corM)):\n",
    "            self.corM[i][i] = -1\n",
    "        \n",
    "    def printChosenMatrix(self, chosenMatrix):\n",
    "        if chosenMatrix == \"corM_A\":\n",
    "            print(\"corM_A:\", end=\"\\n\")\n",
    "            for row in self.corM_A:\n",
    "                print(row, end=\"\\n\")\n",
    "                \n",
    "        if chosenMatrix == \"corM_A_norm\":\n",
    "            print(\"corM_A_norm:\", end=\"\\n\")\n",
    "            for row in self.corM_A_norm:\n",
    "                print(row, end=\"\\n\")\n",
    "                \n",
    "        if chosenMatrix == \"corM_transposed\":\n",
    "            print(\"corM_transposed:\", end=\"\\n\")\n",
    "            for row in self.corM_transposed:\n",
    "                print(row, end=\"\\n\")         \n",
    "        \n",
    "        if chosenMatrix == \"corM\":\n",
    "            print(\"corM:\", end=\"\\n\")\n",
    "            for row in self.corM:\n",
    "                print(row, end=\"\\n\")       \n",
    "                     \n",
    "                     \n",
    "                     \n",
    "### SOME DEBUG\n",
    "dictionary = Dictionary()\n",
    "print(dictionary.terms[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Load files: here we load some example collection of documents. RAW_DOCUMENTS = just strings. Check if the documents are loaded correctly (e.g., print RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David W. Aha:  Machine Learning Page\n",
      " Machine Learning Resources. Suggestions welcome. ... (WizRule); ZDM Scientific\n",
      " Ltd. Conference Announcements. Courses on Machine Learning. Data Repositories. ... \n",
      " Description: Comprehensive machine learning resources from Applications to Tutorials.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RAW_DOCUMENTS = cm.loadDocuments(\"documents.txt\")\n",
    "### SOME DEBUG\n",
    "print(RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', 'aha', 'machine', 'learning', 'page', 'machine', 'learning', 'resource', 'suggestion', 'welcome', 'wizrule', 'zdm', 'scientific', 'ltd', 'conference', 'announcement', 'course', 'machine', 'learning', 'data', 'repository', 'description', 'comprehensive', 'machine', 'learning', 'resource', 'from', 'application', 'tutorials']\n"
     ]
    }
   ],
   "source": [
    "### SOME DEBUG, JUST RUN; check if (a) common.py is imported correctly and (b) \n",
    "### tokens are correctly derived from some document (e.g., RAW_DOCUMENTS[0])\n",
    "print(cm.simpleTextProcessing(RAW_DOCUMENTS[0], re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Get acquainted with the below class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id, raw_document, dictionary):\n",
    "        self.doc_id = doc_id ### DOC ID, simply 0,1,2,3....\n",
    "        self.raw_document = raw_document ### raw data, i.e., string data\n",
    "        self.dictionary = dictionary # reference to the dictionary\n",
    "        \n",
    "        ### DOCUMENT REPRESENTATIONS\n",
    "        self.tokens = cm.simpleTextProcessing(raw_document, re) ### get terms\n",
    "        self.bow = [] # Bag Of Words (BOW - number of term occurences)\n",
    "        self.tf = [] # TF representation\n",
    "        self.tf_idf = [] # TF-IDF representation\n",
    "        \n",
    "        self.bow_lenght = 0\n",
    "        self.bow_dict = {}\n",
    "        self.tf_dict = {}\n",
    "\n",
    "    ### complete this method; it should compute a BOW representation\n",
    "    def computeBOW_Representation(self):\n",
    "        \n",
    "        for term in self.dictionary.terms:\n",
    "            self.bow_dict[term] = 0\n",
    "            \n",
    "        for token in self.tokens:\n",
    "            if token in self.bow_dict:\n",
    "                self.bow_dict[token] += 1\n",
    "        \n",
    "        self.bow = self.bow_dict.items()\n",
    "        self.bow_lenght = len(self.bow)\n",
    "        \n",
    "        \n",
    "    ### complete this method; it should compute a TF representation\n",
    "    ### el: (number of times t occurs in d / total number of terms in d)\n",
    "    def computeTF_Representation(self):\n",
    "        denominator = self.bow_lenght\n",
    "        for term in self.bow:\n",
    "            self.tf.append(term[1] / denominator)\n",
    "            self.tf_dict[term[0]] = term[1] / denominator\n",
    "            \n",
    "    ### complete this method; it should compute a TFxIDF representation \n",
    "    ### (important: it should not be run before dictionary.idfs are not computed!)\n",
    "    ### TFxIDF(t,d,D) = TF(t,d) * IDF(t,D)\n",
    "    def computeTF_IDF_Representation(self):\n",
    "        for term0, term1 in self.tf_dict.items():\n",
    "            self.tf_idf.append(term1*self.dictionary.idfs_dict[term0])\n",
    "                     \n",
    "    def computeRepresentations(self):\n",
    "        self.computeBOW_Representation()\n",
    "        self.computeTF_Representation()\n",
    "        self.computeTF_IDF_Representation()\n",
    "    \n",
    "documents = [Document(i, RAW_DOCUMENTS[i], dictionary) for i in range(len(RAW_DOCUMENTS))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Compute IDFs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least common words:  [['working', 1.9444826721501687], ['www', 1.9444826721501687], ['york', 1.9444826721501687], ['young', 1.9444826721501687], ['zdm', 1.9444826721501687]] \n",
      "\n",
      "Most common words:  [['learning', 0.009984220906600923], ['machine', 0.009984220906600923], ['the', 0.1962946451439682], ['and', 0.3212333817522682], ['description', 0.3881801713828814]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### COMPUTE IDFS HERE (FINISH THE PROPER METHOD OF THE DICTIONARY CLASS - DO NOT FORGET TO RE-RUN THE CELL)\n",
    "dictionary.computeIDFs(documents)\n",
    "\n",
    "### SOME DEBUG\n",
    "res = [[dictionary.terms[i], dictionary.idfs[i]] for i in range(len(dictionary.terms))]\n",
    "res.sort(key = lambda x: x[1])\n",
    "\n",
    "# LEAST COMMON WORDS - HIGH IDF\n",
    "print(\"Least common words: \", res[-5:], \"\\n\")\n",
    "\n",
    "# MOST COMMON WORDS - LOW IDF\n",
    "print(\"Most common words: \", res[:5], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5) Compute the document representations (for each document run computeRepresentations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('learning', 4)\n",
      "('machine', 4)\n"
     ]
    }
   ],
   "source": [
    "for d in documents: \n",
    "    d.computeRepresentations()\n",
    "    \n",
    "### SOME DEBUG (you should see some 4s - which terms are these?)\n",
    "#print(documents[0].bow)\n",
    "for tuple in documents[0].bow:\n",
    "    if tuple[1] == 4:\n",
    "        print(tuple, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6) Finish the below method. It should compute and return a cosine similarity (v1 and v2 are two vectors - tf-idf representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from numpy import dot\n",
    "# from numpy.linalg import norm\n",
    "def getSimilarity(v1, v2):\n",
    "    #return dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    return 1 - spatial.distance.cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7) Run the below script for different queries. getTopResults seeks for documents being the most similar/relevant to the query. Do you find the results satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#query = \"machine learning\"\n",
    "#query = \"academic research\"\n",
    "#query = \"international conference\"\n",
    "query = \"international conference washington\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK = 1 WITH SIMILARITY = 0.47426307643971477 | DOC ID = 76\n",
      "ICML 2003\n",
      " The Twentieth International Conference on Machine Learning (ICML-2003).\n",
      " August 21-24, 2003 Washington, DC USA. The Twentieth International ... \n",
      "\n",
      "\n",
      "RANK = 2 WITH SIMILARITY = 0.4212440556739864 | DOC ID = 86\n",
      "International Conferences on Machine Learning and Applications\n",
      " The 2002 International Conferences on Machine Learning and Applications The\n",
      " 2003 International Conferences on Machine Learning and Applications. \n",
      "\n",
      "\n",
      "RANK = 3 WITH SIMILARITY = 0.283795696788981 | DOC ID = 19\n",
      "1998 International Machine Learning Conference\n",
      " (larger version of cover). The Fifteenth International Conference on Machine Learning.\n",
      " The on-line schedule for ICML-98 contains links to many of the papers. ... \n",
      " Description: The Fifteenth International Conference on Machine Learning. July 24-27, 1998 in Madison, Wisconsin.\n",
      "\n",
      "\n",
      "RANK = 4 WITH SIMILARITY = 0.20004541104625062 | DOC ID = 41\n",
      "ICML-2000\n",
      " Seventeenth International Conference on Machine Learning. Stanford University. ... Tutorials\n",
      " on Commercial Applications of Machine Learning and Data Mining [new]. ... \n",
      " Description: Seventeenth International Conference on Machine Learning. Stanford University June 29-July 2, 2000.\n",
      "\n",
      "\n",
      "RANK = 5 WITH SIMILARITY = 0.195603154983932 | DOC ID = 39\n",
      "ICML2002 - Sydney\n",
      " The Nineteenth International Conference on Machine Learning (ICML-2002). ... Previous\n",
      " meetings on machine learning: ICML-2001, ICML-2000, ICML-99 , ICML-98. ... \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTopResults(query, documents, dictionary, similarity, top = 5):\n",
    "    # construct document which is made of query\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    \n",
    "    # in each of documents check cosine similarity to query (here \"machine learning\")\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults(query, documents, dictionary, getSimilarity, top = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Correlation matrix (for 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1) Finish dictionary.computeCorM method (see class Dictionary). It should generate a correlation matrix (correlation between terms).\n",
    "\n",
    "IMPORTANT: although corM[ i ][ i ] (for each i) should be 1.0, set it to -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.         -1.          0.         ...  0.18898224  0.\n",
      "   0.        ]\n",
      " [ 0.          0.         -1.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.18898224  0.         ... -1.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.         -1.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "  -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "### COMPLETE THE computeCorM METHOD (see one of the first cells)\n",
    "dictionary.computeCorM(documents)\n",
    "#\"corM_A\"\n",
    "#\"corM_transposed\"\n",
    "#\"corM\"\n",
    "print(dictionary.corM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2) Finish the below method. For each term in the query (you must parse the query, see getTopResults() method), find another term which is the most correlated with the input term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 0.9700552531915922\n",
      "the 0.6711549305747254\n",
      "description 0.5852726298569305\n",
      "and 0.5653332650783621\n",
      "for 0.44240101088746486\n",
      "page 0.4135841780912734\n",
      "resource 0.40850357863292663\n",
      "research 0.3812617661967056\n",
      "group 0.37935573716407933\n",
      "home 0.35169164008568093\n"
     ]
    }
   ],
   "source": [
    "#query = \"machine\"\n",
    "#query = \"algorithm\"\n",
    "# query = \"learning\"\n",
    "query = \"conference\"\n",
    "# query = \"research\"\n",
    "# query = \"concept\"\n",
    "\n",
    "def suggestKeywords(query, dictionary):\n",
    "    \n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    query_dictionary_index = dictionary.terms.index(query)\n",
    "    \n",
    "    ranking = []\n",
    "    i = 0\n",
    "    for correlation in dictionary.corM[query_dictionary_index]:\n",
    "        ranking.append((dictionary.terms[i], correlation))\n",
    "        i += 1\n",
    "        \n",
    "    ranking.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    for i in ranking[:10]:\n",
    "        print(i[0], i[1])\n",
    "    \n",
    "\n",
    "suggestKeywords(query, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) Rocchio algorithm (for 4.0)\n",
    "\n",
    "- a method for updating a query vector - vector space model must be used to represent documents (TF-IDF vectors)\n",
    "\n",
    "- linear combination of:\n",
    "    * q\n",
    "    * D_r\n",
    "    * D_nr\n",
    "    \n",
    "- new q_m: \n",
    "    * alfa, beta, gamma are weights\n",
    "    * beta (positive feedback) > gamma (negative feedback)\n",
    "    * user assessed many documents? -> beta, gamma should be high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overrightarrow{q_{m}} = \\alpha \\overrightarrow{q} + \\left(\\beta \\cdot \\dfrac{1}{|D_{r}|} \\sum_{\\overrightarrow{D_j} \\in D_{r}} \\overrightarrow{D_j} \\right) - \\left(\\gamma \\cdot \\dfrac{1}{|D_{nr}|} \\sum_{\\overrightarrow{D_j} \\in D_{nr}} \\overrightarrow{D_j} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1) \n",
    "- Firstly, run the below code.  \n",
    "- Observe the results.  \n",
    "- Assume that we do not like the first and the second result (Docs 63 and 77).  \n",
    "- However, assume that docs 29 and 36 are satisfactory. Now, modfify the method.  \n",
    "- It should alter the query vector, according to Rocchio algorithm.   \n",
    "- Check the result for the above considered scenario (relevant docs = 29 and 36; not relevant = 63 and 77).   \n",
    "- Check the results for different values of alpha, beta, and gamma coefficients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[<__main__.Document object at 0x7f8ba64fa400>, 0.008544060298746436], [<__main__.Document object at 0x7f8ba64fa2e0>, 0.0033796012803712916], [<__main__.Document object at 0x7f8ba64fa9a0>, 0.005693313880880901], [<__main__.Document object at 0x7f8ba64fa640>, 0.007849133516984574], [<__main__.Document object at 0x7f8ba64fa1c0>, 0.004857394975292029], [<__main__.Document object at 0x7f8ba64faac0>, 0.0037797308530653773], [<__main__.Document object at 0x7f8ba64fac10>, 0.004417028405588663], [<__main__.Document object at 0x7f8ba64fa580>, 0.009217502327959592], [<__main__.Document object at 0x7f8ba64fa4c0>, 0.002286550969154977], [<__main__.Document object at 0x7f8ba64fad00>, 0.00402377841247803], [<__main__.Document object at 0x7f8ba64fa880>, 0.0], [<__main__.Document object at 0x7f8ba65fddc0>, 0.0027809891929297947], [<__main__.Document object at 0x7f8ba65fdbb0>, 0.006605595469535874], [<__main__.Document object at 0x7f8ba65fdbe0>, 0.008129643070527215], [<__main__.Document object at 0x7f8ba65fd160>, 0.004796145828599663], [<__main__.Document object at 0x7f8ba65fd970>, 0.004641110401550352], [<__main__.Document object at 0x7f8ba65fd670>, 0.004916431186395065], [<__main__.Document object at 0x7f8ba65fd280>, 0.007677024961294632], [<__main__.Document object at 0x7f8ba65fd7f0>, 0.003960834619597398], [<__main__.Document object at 0x7f8ba65fd400>, 0.005439363533858987], [<__main__.Document object at 0x7f8ba65fd9d0>, 0.00922672700431848], [<__main__.Document object at 0x7f8ba65fdc40>, 0.004004615322267369], [<__main__.Document object at 0x7f8ba65fd340>, 0.004759109078053325], [<__main__.Document object at 0x7f8ba65fd6d0>, 0.003226087459306637], [<__main__.Document object at 0x7f8ba65fd8e0>, 0.006399766270824503], [<__main__.Document object at 0x7f8ba65fdb80>, 0.00688411466386285], [<__main__.Document object at 0x7f8ba65fde20>, 0.003329994535557157], [<__main__.Document object at 0x7f8ba65fda90>, 0.005157767984453443], [<__main__.Document object at 0x7f8ba65fda60>, 0.0097138821102285], [<__main__.Document object at 0x7f8ba65fdf70>, 0.009482776611745125], [<__main__.Document object at 0x7f8ba65fd940>, 0.0020321388857905687], [<__main__.Document object at 0x7f8ba65fdaf0>, 0.00482789704700759], [<__main__.Document object at 0x7f8ba65fd7c0>, 0.00515859569607291], [<__main__.Document object at 0x7f8ba65fd850>, 0.005614634202681934], [<__main__.Document object at 0x7f8ba65fdf10>, 0.012340729913628334], [<__main__.Document object at 0x7f8ba65fde50>, 0.00532058760475318], [<__main__.Document object at 0x7f8ba65fd490>, 0.009717895758309947], [<__main__.Document object at 0x7f8ba65fd0a0>, 0.00847774894362674], [<__main__.Document object at 0x7f8ba65fd5b0>, 0.004346624018618517], [<__main__.Document object at 0x7f8ba65fd790>, 0.007498046519841983], [<__main__.Document object at 0x7f8ba65fd460>, 0.006835508854685424], [<__main__.Document object at 0x7f8ba65fdd00>, 0.005751248484062943], [<__main__.Document object at 0x7f8ba65fd040>, 0.0062958468244346255], [<__main__.Document object at 0x7f8ba65fdd60>, 0.00467205371588908], [<__main__.Document object at 0x7f8ba65fd760>, 0.004753838296433277], [<__main__.Document object at 0x7f8ba65fdfd0>, 0.004646339642568775], [<__main__.Document object at 0x7f8ba65fda30>, 0.004311260636893466], [<__main__.Document object at 0x7f8ba65fd610>, 0.0021451603424307297], [<__main__.Document object at 0x7f8b9aac5160>, 0.001998795395103059], [<__main__.Document object at 0x7f8b9aac5cd0>, 0.002709231991883221], [<__main__.Document object at 0x7f8b9aac5640>, 0.0062971814735584], [<__main__.Document object at 0x7f8b9aac5970>, 0.0016724083491503894], [<__main__.Document object at 0x7f8b9aac5190>, 0.0067452457260067655], [<__main__.Document object at 0x7f8b9aac5910>, 0.007338183965715506], [<__main__.Document object at 0x7f8b9aac5670>, 0.006048108614872816], [<__main__.Document object at 0x7f8b9aac5a90>, 0.0030938169262341164], [<__main__.Document object at 0x7f8b9aac52b0>, 0.005974703476403831], [<__main__.Document object at 0x7f8b9aac50d0>, 0.0055713216306990665], [<__main__.Document object at 0x7f8b9aac5130>, 0.004525641102402833], [<__main__.Document object at 0x7f8b9aac5ac0>, 0.006460400064199678], [<__main__.Document object at 0x7f8b9aac55b0>, 0.006461661747458547], [<__main__.Document object at 0x7f8b9aac5be0>, 0.008478825737521567], [<__main__.Document object at 0x7f8b9aac5070>, 0.010455352603751189], [<__main__.Document object at 0x7f8b9aac5ee0>, 0.017145022655810083], [<__main__.Document object at 0x7f8b9aac54f0>, 0.003475485816545887], [<__main__.Document object at 0x7f8b9aac53a0>, 0.002182889600700122], [<__main__.Document object at 0x7f8b9aac59a0>, 0.0061447326224547405], [<__main__.Document object at 0x7f8b9aac5f40>, 0.0046761612473859016], [<__main__.Document object at 0x7f8b9aac56d0>, 0.0051909076308610524], [<__main__.Document object at 0x7f8b9aac5dc0>, 0.006121305711864866], [<__main__.Document object at 0x7f8b9aac5bb0>, 0.0042627606086061975], [<__main__.Document object at 0x7f8b9aac5730>, 0.006256844211384616], [<__main__.Document object at 0x7f8b9aac5280>, 0.004326576178429242], [<__main__.Document object at 0x7f8b9aac5220>, 0.0034537407405884624], [<__main__.Document object at 0x7f8b9aac51c0>, 0.0030934995716142], [<__main__.Document object at 0x7f8b9aac5250>, 0.005958001954732617], [<__main__.Document object at 0x7f8b9aac57f0>, 0.002421764275587601], [<__main__.Document object at 0x7f8b9aac54c0>, 0.012801913843626722], [<__main__.Document object at 0x7f8b9aac57c0>, 0.004806404308316248], [<__main__.Document object at 0x7f8b9aac5d00>, 0.003596321177816608], [<__main__.Document object at 0x7f8b9aac5fa0>, 0.003590273347019668], [<__main__.Document object at 0x7f8b9aac5610>, 0.011069910022798668], [<__main__.Document object at 0x7f8b9aac5b80>, 0.004222703262312777], [<__main__.Document object at 0x7f8b9aac5550>, 0.0022100765114192455], [<__main__.Document object at 0x7f8b9aac53d0>, 0.006092372222003206], [<__main__.Document object at 0x7f8b9aac5400>, 0.0014122710053736665], [<__main__.Document object at 0x7f8b9aac5e50>, 0.00807376426497286], [<__main__.Document object at 0x7f8b9aac5df0>, 0.0031767668960417383]]\n",
      "RANK = 1 WITH SIMILARITY = 0.017145022655810083 | DOC ID = 63\n",
      "AI / Machine Learning Resources\n",
      " AI / Machine Learning Resources. General Machine Learning. The Journal\n",
      " of Machine Learning. MLnet Machine Learning Archive at GMD. The ... \n",
      "\n",
      "\n",
      "RANK = 2 WITH SIMILARITY = 0.012801913843626722 | DOC ID = 77\n",
      "Machine Learning\n",
      " Machine Learning. Machine Learning Home Page (Editor) Machine Learning Home\n",
      " Page (Publisher) Machine Learning Online by Kluwer Academic Publishers: ... \n",
      "\n",
      "\n",
      "RANK = 3 WITH SIMILARITY = 0.012340729913628334 | DOC ID = 34\n",
      "Machine Learning\n",
      " Machine Learning. Related Sites. Machine Learning Resources courtesy\n",
      " of David Aha A Machine Learning Tutorial a good overview of the ... \n",
      "\n",
      "\n",
      "RANK = 4 WITH SIMILARITY = 0.011069910022798668 | DOC ID = 81\n",
      "Oxford University Machine Learning Group\n",
      " Machine Learning at the Computing Laboratory. ... Logic Programming and\n",
      " Learning and Intelligent Systems. Other Machine Learning Groups. ... \n",
      "\n",
      "\n",
      "RANK = 5 WITH SIMILARITY = 0.010455352603751189 | DOC ID = 62\n",
      "Open Directory - Computers: Artificial Intelligence: Machine ... \n",
      " ... David W. Aha: Machine Learning Page - Comprehensive machine learning\n",
      " resources from Applications to Tutorials. Machine Learning ... \n",
      "\n",
      "\n",
      "RANK = 6 WITH SIMILARITY = 0.009717895758309947 | DOC ID = 36\n",
      "Machine Learning Group\n",
      " [ Bristol CS | Index | Research | ML group | Student projects ] Machine\n",
      " Learning Research Group. Overview. The Machine Learning Research ... \n",
      " Description: Research on higher-order concept learning, inductive logic programming, multi-agent learning systems,...\n",
      "\n",
      "\n",
      "RANK = 7 WITH SIMILARITY = 0.0097138821102285 | DOC ID = 28\n",
      "Machine Learning\n",
      " 6.858/18.428: Machine Learning. Available Lecture Notes. ... Defining models for\n",
      " machine learning. Learning conjunctions in the mistake-bounded model. ... \n",
      "\n",
      "\n",
      "RANK = 8 WITH SIMILARITY = 0.009482776611745125 | DOC ID = 29\n",
      "Machine Learning\n",
      " 6.858/18.428: Machine Learning. ... This course deals with the following topics:\n",
      " Formal models of machine learning; Learning concepts from examples; ... \n",
      "\n",
      "\n",
      "RANK = 9 WITH SIMILARITY = 0.00922672700431848 | DOC ID = 20\n",
      "Machine Learning\n",
      " Machine Learning, THE ... Machine learning refers to a system capable of\n",
      " the autonomous acquisition and integration of knowledge. This ... \n",
      "\n",
      "\n",
      "RANK = 10 WITH SIMILARITY = 0.009217502327959592 | DOC ID = 7\n",
      "15-681 and 15-781 Machine Learning\n",
      " Machine Learning, 15:681 and 15:781, Fall 1998. ... This course covers the theory\n",
      " and practice of machine learning from a variety of perspectives. ... \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTopResults_Rocchio(query, \n",
    "                          documents, \n",
    "                          dictionary, \n",
    "                          similarity, \n",
    "                          rel_docs = [29, 36],\n",
    "                          nrel_docs = [63, 77],\n",
    "                          alpha = 0.5,\n",
    "                          beta = 0.3,\n",
    "                          gamma = 0.2,\n",
    "                          top = 10):\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    ##### TODO: MODIFY qd.tf_idf HERE\n",
    "    \n",
    "    #####\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    print(ranks)\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults_Rocchio(\"machine learning\", documents, dictionary, getSimilarity, top = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) WordNet (for 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1) Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/install.html\n",
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download()\n",
    "\n",
    "https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: synset = (from wiki) (information science) A set of one or more synonyms that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2) Display sysents for \"machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/kasia/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/kasia/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3567/2776160346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'machine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/kasia/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "wn.synsets('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3) Display all definitions (.definition()) for synsets (machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4) For each synset (machine), display its hypernym (a word with a broad meaning constituting a category into which words with more specific meanings fall; a superordinate. For example, colour is a hypernym of red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See: http://www.nltk.org/howto/wordnet.html\n",
    "for more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
